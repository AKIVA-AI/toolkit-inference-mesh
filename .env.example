# AKIVA Inference Mesh - Environment Configuration Template
# Copy this file to .env and update with your values

# ============================================================================
# DEPLOYMENT CONFIGURATION
# ============================================================================

# Environment: development, staging, production
AKIVA_ENV=development

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Model to serve (e.g., "meta-llama/Llama-3.1-8B", "deepseek-ai/DeepSeek-V3")
MODEL_NAME=meta-llama/Llama-3.1-8B

# Model path (if using local model files)
# MODEL_PATH=/path/to/model

# Hugging Face token (for gated models)
# HF_TOKEN=your_huggingface_token_here

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================

# Host to bind to
HOST=0.0.0.0

# Port for HTTP API
PORT=8000

# Number of worker processes
WORKERS=1

# ============================================================================
# INFERENCE CONFIGURATION
# ============================================================================

# Backend: sglang, vllm, mlx
BACKEND=sglang

# Maximum batch size
MAX_BATCH_SIZE=32

# Maximum sequence length
MAX_SEQ_LEN=4096

# GPU memory utilization (0.0-1.0)
GPU_MEMORY_UTILIZATION=0.9

# Tensor parallel size (number of GPUs)
TENSOR_PARALLEL_SIZE=1

# Pipeline parallel size (number of nodes)
PIPELINE_PARALLEL_SIZE=1

# ============================================================================
# P2P NETWORKING (for distributed inference)
# ============================================================================

# Enable P2P networking
P2P_ENABLED=true

# P2P listen address
P2P_LISTEN_ADDR=/ip4/0.0.0.0/tcp/18080

# Initial peers (comma-separated multiaddrs)
# P2P_INITIAL_PEERS=/dns4/bootstrap.example.com/tcp/18080/p2p/12D3KooW...

# Relay servers (comma-separated multiaddrs)
# P2P_RELAY_SERVERS=/dns4/relay.example.com/tcp/18080/p2p/12D3KooW...

# ============================================================================
# AKIVA INTEGRATION
# ============================================================================

# Enable AKIVA event logging
AKIVA_EVENT_LOG_ENABLED=true

# Event log file path
AKIVA_EVENT_LOG_PATH=./logs/inference_events.jsonl

# Cost per 1K tokens (USD) - for cost tracking
AKIVA_COST_PER_1K_TOKENS=0.001

# Tenant ID (for multi-tenant deployments)
# AKIVA_TENANT_ID=default

# Project ID (for project-level tracking)
# AKIVA_PROJECT_ID=default

# ============================================================================
# MONITORING & OBSERVABILITY
# ============================================================================

# Enable Prometheus metrics
METRICS_ENABLED=true

# Metrics port
METRICS_PORT=9090

# Enable request tracing
TRACING_ENABLED=false

# Jaeger endpoint (if tracing enabled)
# JAEGER_ENDPOINT=http://localhost:14268/api/traces

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Enable KV cache
KV_CACHE_ENABLED=true

# KV cache size (MB)
KV_CACHE_SIZE=4096

# Enable prefix caching
PREFIX_CACHE_ENABLED=true

# Enable continuous batching
CONTINUOUS_BATCHING=true

# Request timeout (seconds)
REQUEST_TIMEOUT=300

# ============================================================================
# SECURITY
# ============================================================================

# API key for authentication (optional)
# API_KEY=your_secure_api_key_here

# Enable CORS
CORS_ENABLED=true

# Allowed CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# Enable rate limiting
RATE_LIMIT_ENABLED=true

# Rate limit: requests per minute
RATE_LIMIT_RPM=100

# ============================================================================
# ADVANCED CONFIGURATION
# ============================================================================

# Enable speculative decoding
SPECULATIVE_DECODING=false

# Enable quantization (int8, int4, awq, gptq)
# QUANTIZATION=int8

# Enable flash attention
FLASH_ATTENTION=true

# Enable chunked prefill
CHUNKED_PREFILL=true

# Scheduler policy: fcfs, priority, fair
SCHEDULER_POLICY=fcfs

